{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SYSC4415-Assignment2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO1625d9l+CD5amkjuLurIW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielKyr/NeonateLabeler/blob/main/SYSC4415_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFHe6XEVfK5J"
      },
      "source": [
        "Adapted from [this tutorial](https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAYkQlXpd46U"
      },
      "source": [
        "#Run this cell to be able to access required data\n",
        "#Follow the instructions displayed in the output\n",
        "\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCoeLv_ae1KB"
      },
      "source": [
        "# Introduction\n",
        "Face detection is a computer vision problem that involves finding faces in photos. It is a trivial problem for humans to solve and has been solved reasonably well by classical feature-based techniques, such as the cascade classifier (see below). More recently deep learning methods have achieved state-of-the-art results on standard benchmark face detection datasets. One example is the Multi-task Cascade Convolutional Neural Network, or MTCNN for short.\n",
        "\n",
        "<img src=\"http://kpzhang93.github.io/SPL/stylesheets/image007.png\" width=\"500\">\n",
        "\n",
        "In this assignment, you will discover how to perform face detection in Python using classical and deep learning models.\n",
        "\n",
        "The two main approaches to face recognition: \n",
        "\n",
        "\n",
        "1.   Feature-based: use hand-crafted filters to search for and detect faces\n",
        "2.   Deep learning: learn holistically how to extract faces from the entire image\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF_I8e_Hcm0l"
      },
      "source": [
        "# Part 1) Feature-based Cascaded Classifier\n",
        "Feature-based face detection algorithms are fast and effective and have been used successfully for decades. Perhaps the most successful example is a technique called cascade classifiers first described by Paul Viola and Michael Jones and their 2001 paper titled “Rapid Object Detection using a Boosted Cascade of Simple Features.”\n",
        "\n",
        "In the paper, the AdaBoost model is used to learn a range of very simple or weak features in each face, that together provide a robust classifier. The models are then organized into a hierarchy of increasing complexity, called a “cascade”. Simpler classifiers operate on candidate face regions directly, acting like a coarse filter, whereas complex classifiers operate only on those candidate regions that show the most promise as faces.\n",
        "\n",
        "We are going to use a modern implementation of the Classifier Cascade face detection algorithm as provided in the OpenCV library. The benefit of this implementation is that it provides pre-trained face detection models, and provides an interface to train a model on your own dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CHaj08fze65"
      },
      "source": [
        "## Load test images and pre-trained models\n",
        "Run the below cell to download some test images and the pre-trainined models. Add your own test image by pasting a link to `image_3_url`. Find an image with frontal faces of a group of people that you think will be difficult for the classifier. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPdauUCze2_U",
        "outputId": "dca7edb4-1301-4e3d-e864-fd24424ac89e"
      },
      "source": [
        "import urllib.request\n",
        "model1_url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\"\n",
        "urllib.request.urlretrieve(model1_url, \"haarcascade_frontalface_default.xml\")\n",
        "model2_url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_eye.xml\"\n",
        "urllib.request.urlretrieve(model2_url, \"haarcascade_eye.xml\")\n",
        "image1_url = \"https://thumbor.forbes.com/thumbor/fit-in/1200x0/filters%3Aformat%28jpg%29/https%3A%2F%2Fspecials-images.forbesimg.com%2Fimageserve%2F6052170a9df9bdf69d63f201%2F0x0.jpg\"\n",
        "urllib.request.urlretrieve(image1_url, \"test_1.jpg\")\n",
        "image2_url = \"https://pbs.twimg.com/media/BhxWutnCEAAtEQ6.jpg\"\n",
        "urllib.request.urlretrieve(image2_url, \"test_2.jpg\")\n",
        "image3_url = \"\"\n",
        "urllib.request.urlretrieve(image3_url, \"test_3.jpg\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('test_2.jpg', <http.client.HTTPMessage at 0x7f32c0548990>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efSf5vIO2hCx"
      },
      "source": [
        "## Q1) Face detection (3)\n",
        "Load the face detection model (\"haarcascade_frontalface_default.xml\") and run it on the three test images. \n",
        "**Hint: see [documentation](https://docs.opencv.org/4.5.3/d9/d80/classcv_1_1cuda_1_1CascadeClassifier.html) or [tutorial](https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html)**\n",
        "\n",
        "\n",
        "\n",
        "1.   Plot the overlayed bounding boxes of the detections for each image\n",
        "2.   For \"test_2.jpg\" adjust the parameters `minNeighbors` and `scaleFactor`. Describe what the parameters control and how they affect the face detection. Give a set of parameters that improve the detection over the default values.\n",
        "3. Comment on the performance of the detector on the image you selected for \"test_3.jpg\". Describe the failure points of the detector. Apply the same parameters you selected in *Q1.2)*.  Does it improve the performance?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yf7xinQiiXAx"
      },
      "source": [
        "#Function to load classifier\n",
        "from cv2 import CascadeClassifier\n",
        "#Solution here:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of2TVTOCAdfd"
      },
      "source": [
        "*Discussion here:*\n",
        "\n",
        "*Q1.2)*\n",
        "\n",
        "*Q1.3)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVphJPcDcld3"
      },
      "source": [
        "## Q2) Face Landmark Detection (3)\n",
        "Load the eye detection model (\"haarcascade_eye.xml\") and run it on \"test_1.jpg\".\n",
        "\n",
        "\n",
        "1.   Plot the overlayed bounding boxes of the detections\n",
        "2.   Comment on the performance of this task on the test image and describe the failure points of the detector. Based on this performance, do you think this is a more difficult task than face detection, why or why not?\n",
        "3. Give a set of parameters that improve the detection over the default values. What improvement is seen from changing the parameters?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-vKzkbtA_r4"
      },
      "source": [
        "#Solution here:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nga36x-gBJ8x"
      },
      "source": [
        "*Discussion here:*\n",
        "\n",
        "*Q2.2)*\n",
        "\n",
        "*Q2.3)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ25F--BiXi-"
      },
      "source": [
        "# Part 2) Deep Learning - MTCNN\n",
        "Multi-task Cascaded Convolutional Networks (MTCNN) is a framework developed as a solution for both face detection and face alignment. The process consists of three stages of convolutional networks that are able to recognize faces and landmark location such as eyes, nose, and mouth.\n",
        "The [paper](https://arxiv.org/ftp/arxiv/papers/1604/1604.02878.pdf) proposes MTCNN as a way to integrate both tasks (recognition and alignment) using multi-task learning.\n",
        "\n",
        "\n",
        "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2019/03/Collage-Students-Photograph-with-Bounding-Boxes-and-Facial-Keypoints-Drawn-For-Each-Detected-Face-using-MTCNN.png\" width=\"500\">\n",
        "\n",
        "[Source](https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEz9f7QOrf4f"
      },
      "source": [
        "## Q3) MTCNN Architecture (1)\n",
        "\n",
        "Skim the MTCNN paper. Name and describe the three stages of MTCNN, provide the equation (latex format) for the loss function of each task and state the type of problem (i.e. regression or classification) for each loss function. Provide your answer in the cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qSgtAgKuRyP"
      },
      "source": [
        "*Discussion here:*\n",
        "\n",
        "Stages:\n",
        "1.   \n",
        "2.   \n",
        "3.  \n",
        "\n",
        "Loss functions:\n",
        "1. \n",
        "2. \n",
        "3. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZUZ60FswiO2"
      },
      "source": [
        "## Q4) MTCNN on Test Images (3)\n",
        "\n",
        "Apply MTCNN on the test images from Part 1. Overlay the bounding boxes and the landmarks on each image and plot the results. That is, each image should be displayed with each face bounding box and corresponding landmarks added to the image (similar to the one shown in the Part 2 introduction). Compare these results to those from *Q1)* and discuss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe15ETWG0RQn"
      },
      "source": [
        "#Load MTCNN network\n",
        "!pip install mtcnn\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "#Solution here:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1IE7FHmBtgT"
      },
      "source": [
        "*Discussion here:*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XNCrdfzgA6w"
      },
      "source": [
        "## CelebA Dataset\n",
        "To quantitatively test the performance of the face detections, we need an annotated dataset. We will use the CelebA dataset. \n",
        "\n",
        "CelebFaces Attributes Dataset ([CelebA](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)) is a large-scale face attributes dataset with more than 200K celebrity images. The images in this dataset cover large pose variations and background clutter. It contains annotations for face bounding boxes and 5 landmarks:\n",
        "1. left_eye\n",
        "2. right_eye\n",
        "3. nose\n",
        "4. mouth_left\n",
        "5. mouth_right\n",
        "\n",
        "A subset of these images and their annotations are provided in a zip file on google drive. Run the cell below to download and load the data into the workspace.\n",
        "\n",
        "You can access the image data by: `imgs[#]`\n",
        "\n",
        "You can access the image filename by: `imgs.files[#].split('/')[-1]`\n",
        "\n",
        "You can access elements of annoation dataframe in 3 ways:\n",
        " 1. `anno.loc[image_id,column_name]`\n",
        " 2. `anno.iloc[row_#,column_#]`\n",
        " 3. `anno[column_name][row_#/image_id]`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SefDeDSbaXN4"
      },
      "source": [
        "import io\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from zipfile import ZipFile\n",
        "import skimage.io \n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "#Download  file\n",
        "file_id = \"1Nt9uUZgssHUlwHbe9LFVZ0hhgaIQmcj-\"\n",
        "request = drive_service.files().get_media(fileId=file_id)\n",
        "downloaded = io.BytesIO()\n",
        "downloader = MediaIoBaseDownload(downloaded, request)\n",
        "done = False\n",
        "while done is False:\n",
        "  _, done = downloader.next_chunk()\n",
        "#Extract zip file\n",
        "with ZipFile(downloaded) as zipf:\n",
        "    zipf.extractall()\n",
        "\n",
        "#Load images\n",
        "imgs = skimage.io.collection.ImageCollection('celeb_a_mini/imgs/*.jpg')\n",
        "#Load bounding box\n",
        "df = pd.read_csv('celeb_a_mini/list_bbox_celeba.txt',delim_whitespace=True,header=1).set_index('image_id')\n",
        "bbox = df.loc[[f.split('/')[-1] for f in imgs.files]].copy()\n",
        "#Load landmarks\n",
        "df = pd.read_csv('celeb_a_mini/list_landmarks_celeba.txt',delim_whitespace=True,header=1)\n",
        "landmarks = df.loc[[f.split('/')[-1] for f in imgs.files]].copy()\n",
        "#Combine annotations\n",
        "anno = pd.concat([bbox,landmarks],axis=1)\n",
        "\n",
        "#Plot example image\n",
        "plt.imshow(imgs[10]) #access image data by accessing the array (i.e. imgs[#])\n",
        "plt.show()\n",
        "#Print locations of landmarks\n",
        "print(imgs.files[10].split('/')[-1])\n",
        "print('left-eye coordinates: ({},{})'.format(anno['lefteye_x'][10],anno['lefteye_y'][10]))\n",
        "print('upper left point of bbox: ({},{})'.format(anno['x_1'][10],anno['y_1'][10]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piS2RHfegQQl"
      },
      "source": [
        "#Display dataframe which contains ground truth annotations\n",
        "anno.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BsH5neO81jU"
      },
      "source": [
        "## Intersection over Union\n",
        "One metric to evaluate the accuracy of our face detection is Intersection over Union (IoU). IoU is an evaluation metric used to measure the accuracy of an object detector on a particular dataset. More formally, in order to apply IoU to evaluate an object detector we need:\n",
        "* The ground-truth bounding boxes\n",
        "* The predicted bounding boxes from our model.\n",
        "\n",
        "To calculate IoU we use: \n",
        "\n",
        "<img src=\"https://www.pyimagesearch.com/wp-content/uploads/2016/09/iou_equation.png)\" width=\"500\">\n",
        "\n",
        "[Source](https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ4mI1Hh-KpY"
      },
      "source": [
        "## Q5) Evaluate Performance using IoU (5)\n",
        "\n",
        "Calculate the average IoU across all detections over the CelebA dataset using MTCNN and the Cascaded Classifier (\"haarcascade_frontalface_default.xml\"). Compare the performance of the methods. Show an example of an image with misclassification (IoU < 0.5) for both methods. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeICTKu5B44R"
      },
      "source": [
        "#Solution here:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tP9RexI5B5Hl"
      },
      "source": [
        "*Discussion here:*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FapPwo01mXtf"
      },
      "source": [
        "## Bonus: Mean % Error Landmarks (1)\n",
        "For the correct detections from MTCNN, calculate the average euclidian distance between the predicted landmark with the ground truth landmarks for all landmarks; normalize these values with respect to the inter-ocular distance (ground-truth distance between left and right eyes). Compare the accuracy across the five landmarks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vytxCkaMmXC1"
      },
      "source": [
        "#Solution here:"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}